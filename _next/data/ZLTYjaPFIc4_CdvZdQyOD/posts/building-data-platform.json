{"pageProps":{"slug":"building-data-platform","postData":{"id":"building-data-platform","contentHtml":"<h1 id=\"overview\">Overview</h1>\n<p>本記事ではデータプラットフォームを構築するのに考慮の必要なポイントについてまとめた。</p>\n<p>データプラットフォームに関わる基本的な概念に関しては<a href=\"data-platform-fundamental\">データ処理基盤に関わる基本概念</a>で触れている。</p>\n<h1 id=\"データパイプラインの構築\">データパイプラインの構築</h1>\n<p>データ処理基盤のパイプライン構築には下記のようなステップを踏む</p>\n<ol>\n<li>データ収集</li>\n<li>DWH 作成</li>\n<li>データマート作成</li>\n</ol>\n<figure>\n<img src=\"DWH_Pipeline.png\" alt=\"構築アーキテクチャのイメージ図\">\n</figure>\n<h2 id=\"データ収集\">データ収集</h2>\n<p>データレイクを構築する為にはデータソースからデータを収集する必要がある。\nデータソースは色々な形式が考えられ、RDB である場合や CSV, API, 外部の Web サイトをスクレイピングする場合もあり、データソース毎にそれぞれデータ収集の形式が異なる。</p>\n<p>データ収集において気にするポイント</p>\n<ul>\n<li>負荷\n<ul>\n<li>DB にクエリを発行する場合に負荷が高まらないか。深夜のユーザーが利用しない時間帯に行うべきか、Read Only などマスタ以外のインスタンスを使うべきか</li>\n<li>API の連続呼び出し負荷</li>\n<li>Web サイトの更新負荷</li>\n</ul>\n</li>\n<li>PII <sup><a href=\"#user-content-fn-pii\" id=\"user-content-fnref-pii\" data-footnote-ref=\"\" aria-describedby=\"footnote-label\">1</a></sup>\n<ul>\n<li>PII は含むべきか、マスキングするべきか</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"ファイルデータ\">ファイルデータ</h3>\n<p>CSV や s/s などのデータの取り込み</p>\n<ul>\n<li>ファイルの更新通知を設けることでファイルの作成過程などでの誤読み取りを防ぐ\n<ul>\n<li>GCS などのオブジェクトストレージを用い、トリガー機能を利用すると良い</li>\n</ul>\n</li>\n<li>ファイル内の構造が変わる可能性がある場合にはカラムなどの情報を持つメタデータを別途用いる</li>\n<li>AVRO を用いるとデータの型などを厳密に定義できる\n<ul>\n<li>JSON などと違いデータの中身を確認しづらいというデメリット</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"データのサイズに注意する\">データのサイズに注意する</h4>\n<ul>\n<li>JSON はキーをすべて保持する必要がある</li>\n<li>CSV はキーを毎回書く必要がないため JSON と比べてデータ量が少なくなる</li>\n<li>データ量を減らしたい場合は <a href=\"https://parquet.apache.org/documentation/latest/\">Parquet</a> を用いると良い\n<ul>\n<li>データをテキストでなくバイナリで表現</li>\n<li>列指向でデータを圧縮</li>\n<li>高効率なカラム単位の圧縮と、異なるデータの種類の列に対応する柔軟な符号化方式を採用</li>\n</ul>\n</li>\n<li>BigQuery などは gzip などで圧縮したデータを読み込める</li>\n</ul>\n<h3 id=\"api\">API</h3>\n<p>API を通したデータの取得に関して。</p>\n<ul>\n<li>一定期間の実行回数、Quota に注意する</li>\n<li>API Key の期限切れに注意する</li>\n</ul>\n<h3 id=\"データベース\">データベース</h3>\n<p>データベースを利用したデータ取得に関して。</p>\n<ul>\n<li>データサイズが大きい場合は fetch や、limit &#x26; offset などを利用する</li>\n<li>更新データの データレイク・DWH 内での取り扱いに注意する\n<ul>\n<li>DWH を扱うようなプロダクトは更新処理が苦手な場合が多い。一時保存先を設けて上書き更新するなど工夫が必要な場合がある</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"データ取得のアプローチ\">データ取得のアプローチ</h4>\n<ol>\n<li>SQL\n<ul>\n<li>SQL でデータを直接抽出</li>\n<li>扱うデータ量やクエリ次第では負荷が高まる</li>\n</ul>\n</li>\n<li>エクスポート\n<ul>\n<li>データをファイルにエクスポートしてファイルからデータを取得</li>\n<li>DB 負荷を抑えられる</li>\n<li>データサイズが大きくなりがち</li>\n</ul>\n</li>\n<li>ダンプファイル\n<ul>\n<li>バイナリ形式のためデータサイズが抑えられる</li>\n<li>復元用の DB が必要\n<ul>\n<li>本番 DB には負荷を与えない</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>更新ログ\n<ul>\n<li>MySQL でいうバイナリログなどの更新ログを利用する\n<ul>\n<li>復元用 DB に取り込む</li>\n</ul>\n</li>\n<li>収集するデータ量が少なくなるのが最大のメリット</li>\n<li>仕組みの構築が複雑になるのがデメリット</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"ログ\">ログ</h3>\n<p>ログを利用したデータ取得に関して。</p>\n<p>ログからは以下のような情報が取得できる。</p>\n<ul>\n<li>アクセス先 URL</li>\n<li>アクセス時間</li>\n<li>リクエスト内容</li>\n<li>ユーザーエージェント</li>\n<li>アプリケーションの文脈を含む情報</li>\n</ul>\n<p>ログ情報収集のアプローチとしては以下の選択肢がある。</p>\n<ol>\n<li>アプリケーション Log を普通に出力しログの処理ツールからデータレイクに流し込む\n<ul>\n<li>GCP なら Cloud Logging -> Log Sink -> GCS / BigQuery のような形</li>\n</ul>\n</li>\n<li>アプリケーション側でメッセージキューに対して必要な情報だけ出力する\n<ul>\n<li>Application -> Pub/Sub -> (Dataflow) -> GCS / BigQuery のような形</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"データレイク構築\">データレイク構築</h2>\n<ul>\n<li>データを加工せずそのまま保存する\n<ul>\n<li>データ損失を防ぐ</li>\n<li>機密情報や個人情報は必要に応じてマスキングを行う</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"データの保存方針\">データの保存方針</h3>\n<ul>\n<li>ファイルはオブジェクトストレージに置く\n<ul>\n<li>ライフサイクルを導入し過去のものはティアを下げ利用料金を抑えるなどする</li>\n<li>圧縮しファイルサイズを抑える</li>\n</ul>\n</li>\n<li>CSV や JSON は DB に入れる事も選択肢に入る\n<ul>\n<li>BigQuery などは JSON を扱うのも得意</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"dwh-構築\">DWH 構築</h2>\n<ul>\n<li>行指向の RDB ではなく 列指向の DB の方がデータ抽出・集計に向いている\n<ul>\n<li>自分は現状 BigQuery 一択</li>\n</ul>\n</li>\n<li>列指向の DB は更新などは不向き(出来ないものも有る)ので運用は気をつける</li>\n</ul>\n<h3 id=\"大量データの分散処理\">大量データの分散処理</h3>\n<p>データレイクや DWH から ETL を行う場合には大量のデータを扱うことも珍しくない。\n複数マシンを用いてスケールアウトしやすい <a href=\"https://spark.apache.org/\">Spark</a> や <a href=\"https://flink.apache.org/\">Flink</a> 、 GCP であれば <a href=\"https://cloud.google.com/dataflow\">Dataflow</a> を用いてスケーラブルなアプローチで分散処理を行う。</p>\n<p>パイプライン構築時には予め分散処理を前提にしておくと良い。\n最初はデータ量が少なく必要がないかもしれないが、予め大規模分散処理を行いやすいアーキテクチャで構築すると少量のデータも扱え、後々データが増えた場合にも簡単に対応できる。</p>\n<h1 id=\"アーキテクチャ\">アーキテクチャ</h1>\n<p>データ処理基盤を構築するための代表的なアーキテクチャ</p>\n<h2 id=\"ラムダアーキテクチャ\">ラムダアーキテクチャ</h2>\n<p>バッチ処理(バッチレイヤー)とストリーム処理(スピードレイヤー)に分けてデータを処理する。</p>\n<ul>\n<li>バッチレイヤー (コールドパス) は、すべての受信データを未加工の形式で保存し、データに対してバッチ処理を実行する。 処理の結果はバッチビューとして保存される</li>\n<li>スピードレイヤー (ホットパス) では、リアルタイムでデータを分析する このレイヤーは、精度と引き換えに待機時間が短くなるように設計されている</li>\n<li>バッチレイヤーは効率的なクエリのためにバッチビューにインデックスを付けるサービスレイヤーにフィードし、スピードレイヤーは最新のデータに基づく増分更新でサービスレイヤーを更新する</li>\n</ul>\n<h2 id=\"カッパアーキテクチャ\">カッパアーキテクチャ</h2>\n<p>ラムダアーキテクチャと基本的な目標は同じだが、ストリーム処理システムを使用してすべてのデータが単一のパスを経由するという違いがある。\nラムダアーキテクチャは 2 つの異なる場所をデータが通るため計算ロジックが複雑になるのが課題だったが、それを解決した。</p>\n<h1 id=\"参考\">参考</h1>\n<ul>\n<li><a href=\"https://amzn.to/3q1V4xl\">実践的データ基盤への処方箋〜 ビジネス価値創出のためのデータ・システム・ヒトのノウハウ</a></li>\n<li><a href=\"https://docs.microsoft.com/ja-jp/azure/architecture/data-guide/big-data/\">ビッグ データ アーキテクチャ</a></li>\n<li><a href=\"https://services.google.com/fh/files/misc/bring-data-lakes-and-data-warehouses-together.pdf\">Converging Architectures: Bringing Data Lakes and Data Warehouses Together</a></li>\n<li><a href=\"https://cloud.google.com/architecture/build-a-data-lake-on-gcp\">データレイクとしての Cloud Storage</a></li>\n</ul>\n<section data-footnotes=\"\" class=\"footnotes\"><h2 id=\"footnote-label\" class=\"sr-only\">Footnotes</h2>\n<ol>\n<li id=\"user-content-fn-pii\">\n<p>Personally Identifiable Information、 個人情報 <a href=\"#user-content-fnref-pii\" data-footnote-backref=\"\" class=\"data-footnote-backref\" aria-label=\"Back to content\">↩</a></p>\n</li>\n</ol>\n</section>","title":"データ処理基盤構築プラクティス","date":"2022-03-14","tags":["data","dwh"],"description":"データ処理基盤を構築する","published":true,"engineering":true}},"__N_SSG":true}